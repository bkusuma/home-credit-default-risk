{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score,\n",
    "                             recall_score, f1_score, classification_report, \n",
    "                             ConfusionMatrixDisplay)\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "from category_encoders import HashingEncoder\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iterate through different scikit-learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Cross validate model with Kfold stratified cross val ---------------\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Modeling step Test differents algorithms\n",
    "classifiers = ['AdaBoostClassifier',\n",
    "                'BernoulliNB',\n",
    "                'DummyClassifier']\n",
    "results = []\n",
    "for model in classifiers :\n",
    "    results.append(cross_val_score(model, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"AdaBoostClassifier\",\"BernoulliNB\",\"DummyClassifier\"]})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First \t1\tEnable GPU Runtime:\n",
    "# \t•\tGo to \"Runtime\" -> \"Change runtime type\".\n",
    "# \t•\tSelect \"GPU\" as the hardware accelerator.\n",
    "# \t•\tClick \"Save\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install a specific CUDA version\n",
    "# !apt-get --purge remove cuda nvidia* libnvidia-*\n",
    "# !dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
    "# !apt-get remove cuda-*\n",
    "# !apt autoremove\n",
    "# !apt-get update\n",
    "\n",
    "# # Install CUDA 11.x\n",
    "# !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
    "# !mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "# !wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu1804-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
    "# !dpkg -i cuda-repo-ubuntu1804-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
    "# !cp /var/cuda-repo-ubuntu1804-11-8-local/cuda-*-*.pub /tmp/cuda-*-*.pub\n",
    "# !apt-key add /tmp/cuda-*-*.pub\n",
    "# !apt-get update\n",
    "# !apt-get -y install cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cudf  # this should work without any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# # Google Colab: Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU (musr first choose runtime that is a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving preprocessor for future use\n",
    "to_save = {\"preprocessor\" : preprocessor,\n",
    "           \"porch_func\" : porch_func}\n",
    "filename = \"preprocessor.joblib\"\n",
    "\n",
    "joblib.dump(to_save, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clone the entire repo.\n",
    "# !git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
    "# %cd cloned-repo\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch a single <1MB file using the raw GitHub URL.\n",
    "# !curl --remote-name \\\n",
    "#      -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "#      --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once!\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_prepend = \"../..\"\n",
    "input_path = \"/kaggle/input/home-credit-default-risk/\"\n",
    "# kaggle_notebook_input_path = \"/kaggle/input/home-credit-default-risk/\"\n",
    "\n",
    "# con = sqlite3.connect(local_prepend + input_path + \"home-credit-default-risk.sqlite\") # connect to database\n",
    "# pd.read_csv(local_prepend + input_path + \"application_train.csv\").to_sql(\"application_train\", con, if_exists='append', index=False, index_label=\"SK_ID_CURR\")\n",
    "\n",
    "# application_train = pd.read_csv(local_prepend + input_path + \"application_train.csv\")\n",
    "\n",
    "# bureau_balance = pd.read_csv(local_prepend + input_path + \"bureau_balance.csv\")\n",
    "# bureau = pd.read_csv(local_prepend + input_path + \"bureau.csv\")\n",
    "# credit_card_balance = pd.read_csv(local_prepend + input_path + \"credit_card_balance.csv\")\n",
    "# installments_payments = pd.read_csv(local_prepend + input_path + \"installments_payments.csv\")\n",
    "# POS_CASH_balance = pd.read_csv(local_prepend + input_path + \"POS_CASH_balance.csv\")\n",
    "# previous_application = pd.read_csv(local_prepend + input_path + \"previous_application.csv\")\n",
    "\n",
    "# Description of columns in provided datasets\n",
    "HomeCredit_columns_description = pd.read_csv(local_prepend + input_path + \"HomeCredit_columns_description.csv\", encoding = \"latin\")\n",
    "\n",
    "# # Test Data for later use\n",
    "# application_test = pd.read_csv(local_prepend + input_path + \"application_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HomeCredit_columns_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HomeCredit_columns_description[HomeCredit_columns_description[\"Table\"] == \"application_{train|test}.csv\"][[\"Row\", \"Description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HomeCredit_columns_description[~HomeCredit_columns_description[\"Special\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dfs_to_merge = [bureau, previous_application, POS_CASH_balance, credit_card_balance]\n",
    "\n",
    "# for df in dfs_to_merge:\n",
    "#     application_train = pd.merge(application_train, df, how=\"inner\", on=\"SK_ID_CURR\")\n",
    "\n",
    "# application_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.merge(application_train, bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "application_train = application_train.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "application_train = pd.merge(application_train, previous_application, how=\"left\", on=\"SK_ID_CURR\")\n",
    "application_train = application_train.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "application_train = pd.merge(application_train, POS_CASH_balance, how=\"left\", on=\"SK_ID_CURR\")\n",
    "application_train = application_train.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "application_train = pd.merge(application_train, credit_card_balance, how=\"left\", on=\"SK_ID_CURR\")\n",
    "application_train = application_train.drop_duplicates(subset=[\"SK_ID_CURR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = application_train.select_dtypes(include='object').columns.to_list()\n",
    "\n",
    "for col in object_cols:\n",
    "    print(application_train[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = application_train.select_dtypes(include='number')\n",
    "df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_num = df_num.dropna()\n",
    "# df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_num.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(df_num.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(df_num.corr().where(lambda x: abs(x)>0.5), mask=mask, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = df_num.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = df_num[\"TARGET\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "num_cols = X.columns.to_list()\n",
    "\n",
    "# Create Transformers\n",
    "zero_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# Create tuples\n",
    "impute_tuple = (\"imputation\", zero_imputer, num_cols)\n",
    "# scaler_tuple = (\"scaling\", scaler, num_cols)\n",
    "\n",
    "\n",
    "# Create ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[impute_tuple], remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=True)\n",
    "\n",
    "# Test preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "X_train_proc = preprocessor.transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "X_val_proc = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_balanced = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "\n",
    "logreg_balanced_pipe = make_pipeline(StandardScaler(), logreg_balanced)\n",
    "\n",
    "logreg_balanced_pipe.fit(X_train_proc, y_train)\n",
    "\n",
    "# balanced_results = eval_classification(logreg_balanced_pipe, X_train_proc, y_train, X_test_proc, y_test,\n",
    "#                               model_name=\"Logistic Regression Balanced Class Weights\")\n",
    "\n",
    "# balanced_results\n",
    "\n",
    "print(\"Train\")\n",
    "print(classification_report(y_train, train_pred))\n",
    "print(\"Test\")\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "# param_space = {'n_neighbors': range(3,8)}\n",
    "# knn_gs = GridSearchCV(knn, param_space)\n",
    "# knn_gs.fit(X_train_proc, y_train)\n",
    "# best_knn = knn_gs.best_estimator_\n",
    "# best_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", best_knn)])\n",
    "\n",
    "knn_pipe.fit(X_train_proc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_imputer.fit(X)\n",
    "\n",
    "zero_X = zero_imputer.transform(X)\n",
    "knn_pipe2 = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                      (\"knn\", KNeighborsClassifier(n_neighbors=6))])\n",
    "\n",
    "knn_pipe2.fit(zero_X, y) \n",
    "\n",
    "zero_X_cols = zero_X.columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "clf.fit(X_proc, y)\n",
    "\n",
    "importances = permutation_importance(clf, X_proc, y, random_state=42, scoring=\"roc_auc\")\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.float_format = '{:.4f}'.format # global rounding\n",
    "\n",
    "importance_series = pd.Series(importances.importances_mean, index=X_proc.columns)\n",
    "importance_series.sort_values(ascending=False).round(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_mask = importance_series > 0\n",
    "importance_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp = X_proc.loc[:, importance_mask]\n",
    "X_imp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'\n",
    "\n",
    "\n",
    "# ridge = RidgeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "# ridge.fit(X_train, y_train)\n",
    "\n",
    "# ridge_results = eval_classification(ridge, X_train, y_train, X_test, y_test,\n",
    "#                               model_name=\"Ridge Classifier\")\n",
    "# # , results_frame=rf_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'RidgeClassifierCV' object has no attribute 'predict_proba'\n",
    "\n",
    "\n",
    "# ridge_cv = RidgeClassifierCV(class_weight=\"balanced\", scoring=\"roc_auc\")\n",
    "# ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# ridge_cv_results = eval_classification(ridge_cv, X_train, y_train, X_test, y_test,\n",
    "#                               model_name=\"Ridge Classifier CV\", roc_auc_average=\"macro\")\n",
    "# # , results_frame=rf_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# knn_results = eval_classification(knn, X_train, y_train, X_test, y_test,\n",
    "#                                   model_name=\"K Nearest Neighbors\", results_frame=dt_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_params = {'n_neighbors': range(3,8),\n",
    "#               'leaf_size': [10, 20, 30, 40, 50]}\n",
    "# knn_gs = GridSearchCV(knn, knn_params, scoring=\"roc_auc\")\n",
    "# knn_gs.fit(X_train, y_train)\n",
    "# knn_cv = knn_gs.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "# knn_cv_results = eval_classification(knn_cv, X_train, y_train, X_test, y_test,\n",
    "#                                   model_name=\"K Nearest Neighbors CV\", results_frame=knn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = \"application_test.csv\"\n",
    "test_data = pd.read_csv(local_prepend + input_path + test_data_file)\n",
    "\n",
    "test_data = pd.merge(test_data, bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "test_data = test_data.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "test_data = pd.merge(test_data, previous_application, how=\"left\", on=\"SK_ID_CURR\")\n",
    "test_data = test_data.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "test_data = pd.merge(test_data, POS_CASH_balance, how=\"left\", on=\"SK_ID_CURR\")\n",
    "test_data = test_data.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "test_data = pd.merge(test_data, credit_card_balance, how=\"left\", on=\"SK_ID_CURR\")\n",
    "test_data = test_data.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "\n",
    "ids = test_data.pop(\"SK_ID_CURR\")\n",
    "\n",
    "test_data = test_data[zero_X_cols]\n",
    "\n",
    "zero_test = zero_imputer.transform(test_data)\n",
    "\n",
    "preds = knn_pipe2.predict_proba(zero_test)[:,1]\n",
    "\n",
    "output = pd.DataFrame({\"SK_ID_CURR\": ids,\n",
    "                       \"TARGET\": preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_file = \"sample_submission.csv\"\n",
    "sample_submission_df = pd.read_csv(local_prepend + input_path + sample_submission_file)\n",
    "sample_submission_df[\"TARGET\"] = preds\n",
    "sample_submission_df.to_csv(\"00f_more_merges.csv\", index=False)\n",
    "sample_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a logistic regression\n",
    "# log_reg = LogisticRegression(max_iter=100000, random_state=42)\n",
    "# log_reg.fit(X_train_proc, y_train)\n",
    "# log_reg.predict_proba(X_test_proc)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classification(best_knn, X_train_proc, y_train, X_test_proc, y_test, model_name=\"best knn\", results_frame=None,\n",
    "                        pos_label=1, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         class_weight=\"balanced\",\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "\n",
    "sfs = SequentialFeatureSelector(clf, tol=-0.01156, direction=\"backward\", scoring=\"roc_auc\", n_jobs=-1)\n",
    "sfs.fit(X_proc, y)\n",
    "sfs.get_support()\n",
    "sfs.transform(X_proc).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "X_new = SelectKBest(f_classif, k=\"all\").fit_transform(X_proc, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.difference(X_non_co.columns).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m tuned_clf = \u001b[43mTunedThresholdClassifierCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43m                                       \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroc_auc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m tuned_clf_results = eval_classification(tuned_clf, X_train, y_train, X_test, y_test,\n",
      "\u001b[32m      5\u001b[39m                                   model_name=\u001b[33m\"\u001b[39m\u001b[33mTuned Threshold Classifer\u001b[39m\u001b[33m\"\u001b[39m, results_frame=stacked_clf_results)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/datascience/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1382\u001b[39m     estimator._validate_params()\n",
      "\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n",
      "\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n",
      "\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n",
      "\u001b[32m   1387\u001b[39m     )\n",
      "\u001b[32m   1388\u001b[39m ):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/datascience/lib/python3.12/site-packages/sklearn/model_selection/_classification_threshold.py:135\u001b[39m, in \u001b[36mBaseThresholdClassifier.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n",
      "\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type != \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m    132\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly binary classification is supported. Unknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    133\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator_, \u001b[33m\"\u001b[39m\u001b[33mn_features_in_\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_features_in_ = \u001b[38;5;28mself\u001b[39m.estimator_.n_features_in_\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/datascience/lib/python3.12/site-packages/sklearn/model_selection/_classification_threshold.py:783\u001b[39m, in \u001b[36mTunedThresholdClassifierCV._fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n",
      "\u001b[32m    776\u001b[39m         fit_params_train = _check_method_params(\n",
      "\u001b[32m    777\u001b[39m             X, routed_params.estimator.fit, indices=train_idx\n",
      "\u001b[32m    778\u001b[39m         )\n",
      "\u001b[32m    780\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimator_.fit(X_train, y_train, **fit_params_train)\n",
      "\u001b[32m    782\u001b[39m cv_scores, cv_thresholds = \u001b[38;5;28mzip\u001b[39m(\n",
      "\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m     *\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score_over_thresholds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    785\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprefit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    786\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    787\u001b[39m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    788\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    789\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    790\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    791\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcurve_scorer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_curve_scorer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    792\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    793\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n",
      "\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    796\u001b[39m )\n",
      "\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(np.isclose(th[\u001b[32m0\u001b[39m], th[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m cv_thresholds):\n",
      "\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m    800\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe provided estimator makes constant predictions. Therefore, it is \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    801\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimpossible to optimize the decision threshold.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    802\u001b[39m     )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/datascience/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n",
      "\u001b[32m     72\u001b[39m config = get_config()\n",
      "\u001b[32m     73\u001b[39m iterable_with_config = (\n",
      "\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n",
      "\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n",
      "\u001b[32m     76\u001b[39m )\n",
      "\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/datascience/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n",
      "\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n",
      "\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n",
      "\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n",
      "\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n",
      "\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/datascience/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n",
      "\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n",
      "\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n",
      "\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n",
      "\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n",
      "\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/datascience/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n",
      "\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n",
      "\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n",
      "\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n",
      "\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tuned_clf = TunedThresholdClassifierCV(stacked_clf, random_state=42,\n",
    "                                       scoring=\"roc_auc\", n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "tuned_clf_results = eval_classification(tuned_clf, X_train, y_train, X_test, y_test,\n",
    "                                  model_name=\"Tuned Threshold Classifer\", results_frame=stacked_clf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_non_co = non_co_pipe.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "selector = SelectFromModel(clf, threshold=0, max_features=40)\n",
    "\n",
    "selector.fit(X_train_proc, y_train)\n",
    "\n",
    "X_train_sel = selector.transform(X_train_proc)\n",
    "X_train_sel\n",
    "\n",
    "sel_pipe = Pipeline([(\"preprocessor\", preprocessor),\n",
    "                     (\"selector\", selector),\n",
    "                     (\"model\", RandomForestClassifier(random_state=42))])\n",
    "\n",
    "sel_pipe.fit(X_train, y_train)\n",
    "\n",
    "results_sel = eval_classification(sel_pipe, X_train, y_train, X_test, y_test, model_name=\"RF SelectFromModel\", results_frame=results_imp)\n",
    "\n",
    "results_sel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.000038\n",
    "0.01156"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
